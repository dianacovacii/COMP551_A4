{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceaRbug4qdRV"
   },
   "source": [
    "Diana Covaci, 261 086 280\n",
    "\n",
    "Nicholas Milin, 261 106 314\n",
    "\n",
    "Viktor Allais, 261 148 866\n",
    "\n",
    "Link to write-up draft: https://docs.google.com/document/d/1SFnJJ0C4B64lkwmnU2XbigqCRX71LE-kLrOZSMkxHWE/edit?usp=sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Y05lOL2mperO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn ucimlrepo\n",
    "!pip install -q torchvision gensim\n",
    "!pip install -q tqdm boto3 requests regex sentencepiece sacremoses\n",
    "!pip install -q pytorch-pretrained-bert transformers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import sys\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNyexl4q2GXU"
   },
   "source": [
    "# Task 1: Acquire and pre-process the Web of Science Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ohVwA7dC2Oh-"
   },
   "outputs": [],
   "source": [
    "#  get text and labels from data folder\n",
    "with open(\"WOS11967/X.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    X = [line.strip() for line in f.readlines()]\n",
    "with open(\"WOS11967/YL1.txt\", \"r\") as f: \n",
    "    y1 = [int(line.strip()) for line in f.readlines()]\n",
    "with open(\"WOS11967/YL2.txt\", \"r\") as f: \n",
    "    y2 = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "labels = {0:\"Computer Science\", 1:\"Electrical Engineering\", 2:\"Psychology\", 3:\"Mechanical Engineering\", 4:\"Civil Engineering\", 5:\"Medical Science\",6:\"Biochemistry\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre-processing helper\n",
    "def clean_and_tokenize(text): \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9(){}\\[\\].,!^:;\\-_/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "def tokens_to_ids(tokens, word2idx): \n",
    "    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11967\n",
      "132677\n",
      "torch.Size([11967, 300, 100])\n"
     ]
    }
   ],
   "source": [
    "# -- LSTM pre-processing pipeline --\n",
    "# use word2vec becuase of dense semantic embeddings, faster convergence, capturing similarities, reducing sparsity and good flexibility\n",
    "\n",
    "# initialize seed values for stable outcomes\n",
    "rn.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "# clean and tokenize text\n",
    "tokenized_X_LSTM = [clean_and_tokenize(line) for line in X]\n",
    "print(len(tokenized_X_LSTM))\n",
    "# get word frequencies within the vocabulary\n",
    "word_frequency = Counter(word for line in tokenized_X_LSTM for word in line)\n",
    "\n",
    "# didn't implement a maximum vocabulary size, but given size X can take word_frequency.most_common(X-2) to avoid rarest words if improves model performance\n",
    "idx2word = ['<PAD>', '<UNK>'] + list(word_frequency.keys())\n",
    "word2idx = {word:idx for idx, word in enumerate(idx2word)}\n",
    "vocab_size = len(word2idx)\n",
    "print(vocab_size)\n",
    "\n",
    "# produce sequences\n",
    "MAX_LEN = 300   # larger than the average abstract of a scientific paper\n",
    "sequences = [tokens_to_ids(line, word2idx) for line in tokenized_X_LSTM]\n",
    "full_sequences = [seq[:MAX_LEN] + [0]*(MAX_LEN-len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN] for seq in sequences]\n",
    "\n",
    "embedding_dim = 100   # can make larger if this doesn't capture enough complexity\n",
    "# increase min_count if too much noise for model\n",
    "w2v_model = Word2Vec(sentences=tokenized_X_LSTM, vector_size=embedding_dim, min_count=1, sg=1)\n",
    "\n",
    "# build embedding matrix\n",
    "embedding_matrix = np.random.normal(size=(vocab_size, embedding_dim)) * 0.01\n",
    "for word, idx in word2idx.items(): \n",
    "    if word in w2v_model.wv: \n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "\n",
    "# prepare LSTM input\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "X_tensor = torch.tensor(full_sequences, dtype=torch.long)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "embedded = embedding_layer(X_tensor)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11967, 128])\n"
     ]
    }
   ],
   "source": [
    "# BERT -- to do, theres a tutorial that you can use\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "encoding = tokenizer(X, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "print(encoding['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#with torch.no_grad(): \n",
    "#    outputs = bert_model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "# embeddings = outputs.last_hidden_state\n",
    "# cls_embeddings = embeddings[:, 0, :]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJqbgZ0WuctUjsLngi6WIZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
