{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceaRbug4qdRV"
   },
   "source": [
    "Diana Covaci, 261 086 280\n",
    "\n",
    "Nicholas Milin, 261 106 314\n",
    "\n",
    "Viktor Allais, 261 148 866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y05lOL2mperO"
   },
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn ucimlrepo\n",
    "!pip install -q torchvision gensim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNyexl4q2GXU"
   },
   "source": [
    "# Task 1: Acquire and pre-process the Web of Science Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ohVwA7dC2Oh-"
   },
   "outputs": [],
   "source": [
    "#  get text and labels from data folder\n",
    "with open(\"WOS11967/X.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    X = [line.strip() for line in f.readlines()]\n",
    "with open(\"WOS11967/YL1.txt\", \"r\") as f: \n",
    "    y1 = [int(line.strip()) for line in f.readlines()]\n",
    "with open(\"WOS11967/YL2.txt\", \"r\") as f: \n",
    "    y2 = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "labels = {0:\"Computer Science\", 1:\"Electrical Engineering\", 2:\"Psychology\", 3:\"Mechanical Engineering\", 4:\"Civil Engineering\", 5:\"Medical Science\",6:\"Biochemistry\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre-processing helper\n",
    "def clean_and_tokenize(text): \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9(){}\\[\\].,!^:;\\-_/]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "def tokens_to_ids(tokens, word2idx): \n",
    "    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11967\n",
      "132677\n",
      "torch.Size([11967, 300, 100])\n"
     ]
    }
   ],
   "source": [
    "# -- LSTM pre-processing pipeline --\n",
    "# use word2vec becuase of dense semantic embeddings, faster convergence, capturing similarities, reducing sparsity and good flexibility\n",
    "\n",
    "# clean and tokenize text\n",
    "tokenized_X = [clean_and_tokenize(line) for line in X]\n",
    "print(len(tokenized_X))\n",
    "# get word frequencies within the vocabulary\n",
    "word_frequency = Counter(word for line in tokenized_X for word in line)\n",
    "\n",
    "# didn't implement a maximum vocabulary size, but given size X can take word_frequency.most_common(X-2) to avoid rarest words if improves model performance\n",
    "idx2word = ['<PAD>', '<UNK>'] + list(word_frequency.keys())\n",
    "word2idx = {word:idx for idx, word in enumerate(idx2word)}\n",
    "vocab_size = len(word2idx)\n",
    "print(vocab_size)\n",
    "\n",
    "# produce sequences\n",
    "MAX_LEN = 300   # larger than the average abstract of a scientific paper\n",
    "sequences = [tokens_to_ids(line, word2idx) for line in tokenized_X]\n",
    "full_sequences = [seq[:MAX_LEN] + [0]*(MAX_LEN-len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN] for seq in sequences]\n",
    "\n",
    "embedding_dim = 100   # can make larger if this doesn't capture enough complexity\n",
    "# increase min_count if too much noise for model\n",
    "w2v_model = Word2Vec(sentences=tokenized_X, vector_size=embedding_dim, min_count=1, sg=1)\n",
    "\n",
    "# build embedding matrix\n",
    "embedding_matrix = np.random.normal(size=(vocab_size, embedding_dim)) * 0.01\n",
    "for word, idx in word2idx.items(): \n",
    "    if word in w2v_model.wv: \n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "\n",
    "# prepare LSTM input\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "X_tensor = torch.tensor(full_sequences, dtype=torch.long)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "embedded = embedding_layer(X_tensor)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT -- to do, theres a tutorial that you can use"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJqbgZ0WuctUjsLngi6WIZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
